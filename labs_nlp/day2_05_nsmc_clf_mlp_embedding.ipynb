{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "224d8000-e1bb-4843-8d1a-c8cc22901ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import koreanize_matplotlib\n",
    "from konlpy.tag import Okt\n",
    "from gensim.models import Word2Vec\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77d57f66-2c8f-441d-b1a6-21ddbcf6c01f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결측치 처리 이후: 10000\n",
      "중복 제거 이후: 10000\n",
      "한글 아닌 문자 제거 이후: 10000\n",
      "리뷰 길이가 짧은 것 제거 : 10000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6928782</td>\n",
       "      <td>쌕기들이 종교랑 보솤ㅋㅋㅋㅋㅋ 그라제 우덜이 미군이 써보지도 완전히 전쟁도 있었당께...</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8330404</td>\n",
       "      <td>정유미 정준영 부부막둥이 윤한부부 나오면 채널로 최초로 부부가 재미없음 막둥이 윤한...</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8525988</td>\n",
       "      <td>측정용 영화임 당신은 마음이 남에게 상처를 못하는 당신은 인문학예술은 당신의 분야가...</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8153927</td>\n",
       "      <td>황당한 쓰레기같은 스토리 수준을 초월한 칭찬해 차라리 디워는 나와서 신기하기라도 도...</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8120918</td>\n",
       "      <td>남자고 아비라면 김윤진 역보다 어미니의 아들입장이라면 저보다 평만보면 공자시구만ㅋㅋ...</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                           document  label  \\\n",
       "0  6928782  쌕기들이 종교랑 보솤ㅋㅋㅋㅋㅋ 그라제 우덜이 미군이 써보지도 완전히 전쟁도 있었당께...      0   \n",
       "1  8330404  정유미 정준영 부부막둥이 윤한부부 나오면 채널로 최초로 부부가 재미없음 막둥이 윤한...      0   \n",
       "2  8525988  측정용 영화임 당신은 마음이 남에게 상처를 못하는 당신은 인문학예술은 당신의 분야가...      0   \n",
       "3  8153927  황당한 쓰레기같은 스토리 수준을 초월한 칭찬해 차라리 디워는 나와서 신기하기라도 도...      0   \n",
       "4  8120918  남자고 아비라면 김윤진 역보다 어미니의 아들입장이라면 저보다 평만보면 공자시구만ㅋㅋ...      1   \n",
       "\n",
       "   word_count  \n",
       "0          41  \n",
       "1          40  \n",
       "2          40  \n",
       "3          40  \n",
       "4          40  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/hongsukyi/Lectures/main/data/nsmc.txt\", sep=\"\\t\")\n",
    "df = df.dropna().drop_duplicates(['document']).reset_index(drop=True)\n",
    "\n",
    "df['word_count'] = df['document'].apply(lambda x: len(str(x).split()))\n",
    "df_sorted = df.sort_values(by='word_count', ascending=False).reset_index(drop=True)\n",
    "\n",
    "df=df_sorted[:10000].copy()\n",
    "\n",
    "df.replace(\"\", float(\"NaN\"), inplace=True)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "print('결측치 처리 이후:',len(df))\n",
    "df = df.drop_duplicates(['document']).reset_index(drop=True)\n",
    "print('중복 제거 이후:',len(df))\n",
    "df['document'] = df['document'].str.replace(r\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", regex=True)\n",
    "print('한글 아닌 문자 제거 이후:',len(df))\n",
    "\n",
    "df['document'] = df['document'].apply(lambda x: ' '.join([token for token in x.split() if len(token) > 2]))\n",
    "print('리뷰 길이가 짧은 것 제거 :',len(df))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f6d24f-1ec9-4a96-aefc-09433b1819fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 짧은 문장 제거\n",
    "okt = Okt()\n",
    "def token_count(text):\n",
    "    return len(okt.morphs(text, stem=True))\n",
    "\n",
    "df = df[df['document'].apply(lambda x: token_count(x) >= 3)].reset_index(drop=True)\n",
    "\n",
    "print(f\"최종 데이터 크기: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83a0669-520c-4716-ba41-a3421a570462",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['의', '가', '이', '은', '들', '는', '좀', '잘', '걍', '과', '도', '를', '으로',\n",
    "             '자', '에', '와', '한', '하다', '을', '적', '로', '인', '만',\n",
    "             '다', '이다', '에서', '되다', '하고', '않다', '못', '고', '안', '것', '나', '그']\n",
    "\n",
    "def clean_tokenize(text):\n",
    "    tokens = okt.morphs(text, stem=True)\n",
    "    return [w for w in tokens if w not in stopwords]\n",
    "\n",
    "df['tokens'] = df['document'].apply(clean_tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0c200a-a3c2-4442-8303-43af8d07b4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_samples=df['tokens']\n",
    "all_tokens = [token for tokens in df_samples for token in tokens]\n",
    "\n",
    "counter = Counter(all_tokens)\n",
    "common_words = counter.most_common(20) \n",
    "words, freqs = zip(*common_words)\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.bar(words, freqs)\n",
    "plt.title(\"Top 7 Most Common Tokens\")\n",
    "plt.xlabel(\"Tokens\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbed9c77-c810-4e81-a3ec-9beb808960a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 패딩을 위한 95% 분위값 출력\n",
    "q95= df['word_count'].quantile([0.95])\n",
    "\n",
    "plt.figure(figsize=(7, 3))\n",
    "plt.hist(df['word_count'], bins=50, color='lightblue', edgecolor='black')\n",
    "\n",
    "\n",
    "plt.axvline(x=q95.iloc[0], color='gray', linestyle='--', label=f'95%: {int(q95.iloc[0])}')\n",
    "plt.title('문서별 단어 수 분포 및 분위수 기준선', fontsize=10)\n",
    "plt.xlabel('단어 수', fontsize=10)\n",
    "plt.ylabel('문서 수', fontsize=10)\n",
    "plt.legend(frameon=False,fontsize=10)\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809789a5-5914-4fd3-98b3-958f3ea6f064",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 35\n",
    "embedding_dim = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f103fd9-88f1-4a99-80f4-41affb28623c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['tokens'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(df['tokens'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "X = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
    "y = np.array(df['label'])\n",
    "print(f\"X shape: {X.shape}, y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06076d31-b104-4f32-85b9-3f1b48f58d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(f\"vocab_size: {vocab_size}\")\n",
    "print(f\"max index in sequences: {np.max(X)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fb5f00-dc97-4fdc-bf07-59b3368fb05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "d1= 128\n",
    "d2= 64\n",
    "drop_rate = 0.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98127edc-68a4-4812-9de8-89f37cb9c911",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_keras_dropout = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(d1, activation='relu'),\n",
    "    Dropout(drop_rate),\n",
    "    Dense(d2, activation='relu'),\n",
    "    Dropout(drop_rate),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_keras = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(d1, activation='relu'),\n",
    "    Dense(d2, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "model_keras_opt = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len),\n",
    "    Dropout(drop_rate),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(d1, activation='relu', kernel_regularizer=regularizers.l2(0.0001)),\n",
    "    Dropout(drop_rate),\n",
    "    Dense(d2, activation='relu', kernel_regularizer=regularizers.l2(0.0001)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a3d639-24b4-4783-a8de-1c47151ce2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = { \n",
    "    'keras_drop': model_keras_dropout,  \n",
    "    'keras': model_keras, \n",
    "    'keras_opt': model_keras_opt }\n",
    "choice = 'keras_opt'\n",
    "model = model_dict[choice]\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdef6c8b-f403-478b-84a3-cb5283faff40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "optimizer = Adam(learning_rate=0.00005)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, epochs=30, batch_size=64,\n",
    "                    validation_split=0.2, callbacks=[early_stop], verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efb5d60-63e0-4e7d-aa4b-d3171e00de05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "plt.plot(history.history['accuracy'], label='Train Acc')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
    "plt.title(f\"Accuracy Curve with {choice}\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Loss\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title(f\"Loss Curve with {choice}\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 최종 평가\n",
    "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {acc:.4f}, Loss: {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdf24ce-46d4-4aa2-b369-8836829c8bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74f1fe3-4e24-4d42-a39a-691c16f4caf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6fe139-1896-456e-a438-91c17951ed2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25e105d-c5b7-4f23-af09-2b68a5481bf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf3",
   "language": "python",
   "name": "tf3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
